{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7936568-b496-4075-b33a-47ba439e897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "Epoch [1/10], Step [100/1145], Loss: 2.2390\n",
      "Epoch [1/10], Step [200/1145], Loss: 2.0429\n",
      "Epoch [1/10], Step [300/1145], Loss: 1.4398\n",
      "Epoch [1/10], Step [400/1145], Loss: 1.0704\n",
      "Epoch [1/10], Step [500/1145], Loss: 0.8886\n",
      "Epoch [1/10], Step [600/1145], Loss: 0.8154\n",
      "Epoch [1/10], Step [700/1145], Loss: 0.7212\n",
      "Epoch [1/10], Step [800/1145], Loss: 0.7132\n",
      "Epoch [1/10], Step [900/1145], Loss: 0.6670\n",
      "Epoch [1/10], Step [1000/1145], Loss: 0.6632\n",
      "Epoch [1/10], Step [1100/1145], Loss: 0.6399\n",
      "Epoch [2/10], Step [100/1145], Loss: 0.5727\n",
      "Epoch [2/10], Step [200/1145], Loss: 0.5770\n",
      "Epoch [2/10], Step [300/1145], Loss: 0.5670\n",
      "Epoch [2/10], Step [400/1145], Loss: 0.5576\n",
      "Epoch [2/10], Step [500/1145], Loss: 0.5138\n",
      "Epoch [2/10], Step [600/1145], Loss: 0.5313\n",
      "Epoch [2/10], Step [700/1145], Loss: 0.5238\n",
      "Epoch [2/10], Step [800/1145], Loss: 0.5125\n",
      "Epoch [2/10], Step [900/1145], Loss: 0.4933\n",
      "Epoch [2/10], Step [1000/1145], Loss: 0.4788\n",
      "Epoch [2/10], Step [1100/1145], Loss: 0.4813\n",
      "Epoch [3/10], Step [100/1145], Loss: 0.4729\n",
      "Epoch [3/10], Step [200/1145], Loss: 0.4557\n",
      "Epoch [3/10], Step [300/1145], Loss: 0.4483\n",
      "Epoch [3/10], Step [400/1145], Loss: 0.4292\n",
      "Epoch [3/10], Step [500/1145], Loss: 0.4354\n",
      "Epoch [3/10], Step [600/1145], Loss: 0.4470\n",
      "Epoch [3/10], Step [700/1145], Loss: 0.4320\n",
      "Epoch [3/10], Step [800/1145], Loss: 0.4308\n",
      "Epoch [3/10], Step [900/1145], Loss: 0.4317\n",
      "Epoch [3/10], Step [1000/1145], Loss: 0.4033\n",
      "Epoch [3/10], Step [1100/1145], Loss: 0.4204\n",
      "Epoch [4/10], Step [100/1145], Loss: 0.3888\n",
      "Epoch [4/10], Step [200/1145], Loss: 0.3942\n",
      "Epoch [4/10], Step [300/1145], Loss: 0.3949\n",
      "Epoch [4/10], Step [400/1145], Loss: 0.4049\n",
      "Epoch [4/10], Step [500/1145], Loss: 0.3912\n",
      "Epoch [4/10], Step [600/1145], Loss: 0.4015\n",
      "Epoch [4/10], Step [700/1145], Loss: 0.4122\n",
      "Epoch [4/10], Step [800/1145], Loss: 0.3931\n",
      "Epoch [4/10], Step [900/1145], Loss: 0.3833\n",
      "Epoch [4/10], Step [1000/1145], Loss: 0.3891\n",
      "Epoch [4/10], Step [1100/1145], Loss: 0.3611\n",
      "Epoch [5/10], Step [100/1145], Loss: 0.3421\n",
      "Epoch [5/10], Step [200/1145], Loss: 0.3651\n",
      "Epoch [5/10], Step [300/1145], Loss: 0.3609\n",
      "Epoch [5/10], Step [400/1145], Loss: 0.3589\n",
      "Epoch [5/10], Step [500/1145], Loss: 0.3730\n",
      "Epoch [5/10], Step [600/1145], Loss: 0.3593\n",
      "Epoch [5/10], Step [700/1145], Loss: 0.3447\n",
      "Epoch [5/10], Step [800/1145], Loss: 0.3587\n",
      "Epoch [5/10], Step [900/1145], Loss: 0.3643\n",
      "Epoch [5/10], Step [1000/1145], Loss: 0.3686\n",
      "Epoch [5/10], Step [1100/1145], Loss: 0.3471\n",
      "Epoch [6/10], Step [100/1145], Loss: 0.3286\n",
      "Epoch [6/10], Step [200/1145], Loss: 0.3399\n",
      "Epoch [6/10], Step [300/1145], Loss: 0.3425\n",
      "Epoch [6/10], Step [400/1145], Loss: 0.3365\n",
      "Epoch [6/10], Step [500/1145], Loss: 0.3261\n",
      "Epoch [6/10], Step [600/1145], Loss: 0.3369\n",
      "Epoch [6/10], Step [700/1145], Loss: 0.3303\n",
      "Epoch [6/10], Step [800/1145], Loss: 0.3353\n",
      "Epoch [6/10], Step [900/1145], Loss: 0.3342\n",
      "Epoch [6/10], Step [1000/1145], Loss: 0.3359\n",
      "Epoch [6/10], Step [1100/1145], Loss: 0.3241\n",
      "Epoch [7/10], Step [100/1145], Loss: 0.3131\n",
      "Epoch [7/10], Step [200/1145], Loss: 0.3154\n",
      "Epoch [7/10], Step [300/1145], Loss: 0.3096\n",
      "Epoch [7/10], Step [400/1145], Loss: 0.3124\n",
      "Epoch [7/10], Step [500/1145], Loss: 0.3464\n",
      "Epoch [7/10], Step [600/1145], Loss: 0.2990\n",
      "Epoch [7/10], Step [700/1145], Loss: 0.3095\n",
      "Epoch [7/10], Step [800/1145], Loss: 0.3186\n",
      "Epoch [7/10], Step [900/1145], Loss: 0.3246\n",
      "Epoch [7/10], Step [1000/1145], Loss: 0.3374\n",
      "Epoch [7/10], Step [1100/1145], Loss: 0.3060\n",
      "Epoch [8/10], Step [100/1145], Loss: 0.3028\n",
      "Epoch [8/10], Step [200/1145], Loss: 0.2929\n",
      "Epoch [8/10], Step [300/1145], Loss: 0.2974\n",
      "Epoch [8/10], Step [400/1145], Loss: 0.3074\n",
      "Epoch [8/10], Step [500/1145], Loss: 0.2970\n",
      "Epoch [8/10], Step [600/1145], Loss: 0.3050\n",
      "Epoch [8/10], Step [700/1145], Loss: 0.3036\n",
      "Epoch [8/10], Step [800/1145], Loss: 0.2856\n",
      "Epoch [8/10], Step [900/1145], Loss: 0.2960\n",
      "Epoch [8/10], Step [1000/1145], Loss: 0.3024\n",
      "Epoch [8/10], Step [1100/1145], Loss: 0.3199\n",
      "Epoch [9/10], Step [100/1145], Loss: 0.2789\n",
      "Epoch [9/10], Step [200/1145], Loss: 0.2596\n",
      "Epoch [9/10], Step [300/1145], Loss: 0.2814\n",
      "Epoch [9/10], Step [400/1145], Loss: 0.2719\n",
      "Epoch [9/10], Step [500/1145], Loss: 0.3035\n",
      "Epoch [9/10], Step [600/1145], Loss: 0.2943\n",
      "Epoch [9/10], Step [700/1145], Loss: 0.2910\n",
      "Epoch [9/10], Step [800/1145], Loss: 0.2979\n",
      "Epoch [9/10], Step [900/1145], Loss: 0.2805\n",
      "Epoch [9/10], Step [1000/1145], Loss: 0.3089\n",
      "Epoch [9/10], Step [1100/1145], Loss: 0.2830\n",
      "Epoch [10/10], Step [100/1145], Loss: 0.2486\n",
      "Epoch [10/10], Step [200/1145], Loss: 0.2424\n",
      "Epoch [10/10], Step [300/1145], Loss: 0.2767\n",
      "Epoch [10/10], Step [400/1145], Loss: 0.2699\n",
      "Epoch [10/10], Step [500/1145], Loss: 0.2584\n",
      "Epoch [10/10], Step [600/1145], Loss: 0.2770\n",
      "Epoch [10/10], Step [700/1145], Loss: 0.2715\n",
      "Epoch [10/10], Step [800/1145], Loss: 0.2828\n",
      "Epoch [10/10], Step [900/1145], Loss: 0.2754\n",
      "Epoch [10/10], Step [1000/1145], Loss: 0.2816\n",
      "Epoch [10/10], Step [1100/1145], Loss: 0.2956\n",
      "Accuracy of the network on the 10000 test images: 88.36432083589429%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import SVHN\n",
    "\n",
    "# Define transformation for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),             # Convert images to grayscale\n",
    "    transforms.Resize((32, 32)),        # Resize images to 32x32\n",
    "    transforms.ToTensor(),              # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize images\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset = SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae4233-7d5f-49d6-9fc9-3b45f6319503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "Select the ResNet model:\n",
      "1. ResNet18\n",
      "2. ResNet50\n",
      "3. ResNet101\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2/3):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1145], Loss: 1.7413\n",
      "Epoch [1/10], Step [200/1145], Loss: 0.8974\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import SVHN\n",
    "from torchvision.models import resnet18, resnet50, resnet101\n",
    "\n",
    "# Define transformation for data preprocessing\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert images to RGB format\n",
    "    transforms.Resize((32, 32)),                  # Resize images to 32x32\n",
    "    transforms.ToTensor(),                        # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))         # Normalize images\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset = SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# ResNet model\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, resnet_type='resnet18', num_classes=10):\n",
    "        super(MyResNet, self).__init__()\n",
    "        if resnet_type == 'resnet18':\n",
    "            self.resnet = resnet18(pretrained=False)\n",
    "        elif resnet_type == 'resnet50':\n",
    "            self.resnet = resnet50(pretrained=False)\n",
    "        elif resnet_type == 'resnet101':\n",
    "            self.resnet = resnet101(pretrained=False)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid ResNet type. Choose between 'resnet18', 'resnet50', and 'resnet101'.\")\n",
    "        \n",
    "        self.resnet.fc = nn.Linear(in_features=self.resnet.fc.in_features, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "    def train_and_evaluate(self, train_loader, test_loader, num_epochs=10):\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "        # Training the model\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        # Evaluating the model\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n",
    "\n",
    "# User input for selecting the ResNet model\n",
    "print(\"Select the ResNet model:\")\n",
    "print(\"1. ResNet18\")\n",
    "print(\"2. ResNet50\")\n",
    "print(\"3. ResNet101\")\n",
    "resnet_choice = int(input(\"Enter your choice (1/2/3): \"))\n",
    "\n",
    "# Model selection based on user input\n",
    "if resnet_choice == 1:\n",
    "    model = MyResNet(resnet_type='resnet18')\n",
    "elif resnet_choice == 2:\n",
    "    model = MyResNet(resnet_type='resnet50')\n",
    "elif resnet_choice == 3:\n",
    "    model = MyResNet(resnet_type='resnet101')\n",
    "else:\n",
    "    print(\"Invalid choice.\")\n",
    "    exit()\n",
    "\n",
    "# Train and evaluate the selected model\n",
    "model.train_and_evaluate(train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5077f-609b-48c7-8273-1e7f0730619e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
